# ft_linear_regression üìà

A comprehensive implementation of **Linear Regression from scratch** using **Gradient Descent**. This project predicts car prices based on their mileage (kilometers driven) without using any machine learning libraries.

## üéØ Project Overview

This project implements the fundamental machine learning algorithm **Linear Regression** to find the relationship between:
- **Input (x)**: Kilometers driven by a car
- **Output (y)**: Price of the car

We use **Gradient Descent** to find the best line that fits through our data points, allowing us to predict car prices for any given mileage.

## üìä What is Linear Regression?

Linear regression finds the "best fit" straight line through data points. The line is represented by the equation:

```
price = a + b √ó kilometers
```

Where:
- **a** (intercept): The base price when kilometers = 0
- **b** (slope): How much the price decreases per kilometer driven

## üßÆ The Mathematics Behind It

### 1. The Hypothesis Function
Our prediction model is:
```
h(x) = Œ∏‚ÇÄ + Œ∏‚ÇÅ √ó x
```
- **Œ∏‚ÇÄ** (theta_zero): The intercept parameter
- **Œ∏‚ÇÅ** (theta_one): The slope parameter
- **x**: Input feature (kilometers)

### 2. Cost Function (Mean Squared Error)
We measure how "wrong" our predictions are using:
```
J(Œ∏‚ÇÄ, Œ∏‚ÇÅ) = (1/m) √ó Œ£(h·µ¢ - y·µ¢)¬≤
```
- **m**: Number of data points
- **h·µ¢**: Our prediction for data point i
- **y·µ¢**: Actual value for data point i

### 3. Gradient Descent
To minimize the cost, we calculate gradients (derivatives) and update parameters:

**For Œ∏‚ÇÄ (intercept):**
```
‚àÇJ/‚àÇŒ∏‚ÇÄ = (1/m) √ó Œ£(h - y)
Œ∏‚ÇÄ = Œ∏‚ÇÄ - Œ± √ó ‚àÇJ/‚àÇŒ∏‚ÇÄ
```

**For Œ∏‚ÇÅ (slope):**
```
‚àÇJ/‚àÇŒ∏‚ÇÅ = (1/m) √ó Œ£((h - y) √ó x)
Œ∏‚ÇÅ = Œ∏‚ÇÅ - Œ± √ó ‚àÇJ/‚àÇŒ∏‚ÇÅ
```

Where **Œ±** (alpha) is the learning rate.

## üîß How the Algorithm Works

### Step 1: Data Preparation
```python
# Read the car data
df = pd.read_csv('data.csv')
x = df['km'].values    # Kilometers (input)
y = df['price'].values # Prices (output)
```

### Step 2: Feature Normalization
**Why normalize?** Raw kilometer values (like 240,000) are much larger than normalized values (like 0.5). This can cause:
- **Overflow errors** in calculations
- **Slow convergence** in gradient descent
- **Numerical instability**

```python
def normalize_features(x):
    return (x - np.mean(x)) / np.std(x)
```

This transforms our data to have:
- **Mean = 0**
- **Standard deviation = 1**

### Step 3: Gradient Descent Algorithm
```python
def gradient_descent(x, y):
    Œ∏‚ÇÄ = 0  # Start with zero intercept
    Œ∏‚ÇÅ = 0  # Start with zero slope
    
    for each iteration:
        # Make predictions
        h = Œ∏‚ÇÄ + Œ∏‚ÇÅ √ó x
        
        # Calculate gradients
        Œ∏‚ÇÄ_gradient = (1/m) √ó Œ£(h - y)
        Œ∏‚ÇÅ_gradient = (1/m) √ó Œ£((h - y) √ó x)
        
        # Update parameters
        Œ∏‚ÇÄ = Œ∏‚ÇÄ - learning_rate √ó Œ∏‚ÇÄ_gradient
        Œ∏‚ÇÅ = Œ∏‚ÇÅ - learning_rate √ó Œ∏‚ÇÅ_gradient
```

### Step 4: Converting Back to Original Scale
Since we normalized our data, we need to convert our Œ∏ parameters back to work with original kilometer/price values:

```python
# Convert slope
b = Œ∏‚ÇÅ √ó (y_std / x_std)

# Convert intercept  
a = y_mean - b √ó x_mean + Œ∏‚ÇÄ √ó y_std
```

Now we can use: `price = a + b √ó kilometers`

## üìÅ Project Structure

```
ft_linear_regression/
‚îú‚îÄ‚îÄ main.py               # Main program execution
‚îú‚îÄ‚îÄ data.csv              # Training data (km, price)
‚îú‚îÄ‚îÄ requirements.txt      # Python dependencies
‚îú‚îÄ‚îÄ Makefile              # Build and run automation
‚îú‚îÄ‚îÄ utils/                # Utility modules
‚îÇ   ‚îú‚îÄ‚îÄ accuracy.py       # Model evaluation metrics
‚îÇ   ‚îú‚îÄ‚îÄ convert.py        # Parameter conversion functions
‚îÇ   ‚îú‚îÄ‚îÄ dashboard.py      # Interactive visualization dashboard
‚îÇ   ‚îú‚îÄ‚îÄ gradient.py       # Gradient descent implementation
‚îÇ   ‚îî‚îÄ‚îÄ plot.py           # Visualization functions
‚îî‚îÄ‚îÄ README.md             # This file
```

## üöÄ How to Run

### Prerequisites
```bash
# Install dependencies
make install
```

### Run the Program
```bash
# Train model and show visualizations
make run

# Predict price for a specific mileage
make predict KM=100000
```

### Command Line Interface
```bash
# View all options
python main.py --help

# Predict a price for a specific mileage
python main.py --prediction 100000
```

### What You'll See

1. **Data visualization**:
   - Scatter plot of original data
   - Regression line visualization
   - Cost convergence plot
   - Predictions vs actual values

2. **Console output** showing:
   - Normalized parameters (Œ∏‚ÇÄ, Œ∏‚ÇÅ)
   - Converted parameters (a, b) 
   - Step-by-step conversion process
   - Final equation
   - Performance metrics (R¬≤, RMSE, MAE, etc.)

3. **Comprehensive dashboard** with:
   - Main regression plot
   - Cost convergence visualization
   - Prediction accuracy visualization
   - Performance metrics summary

## üìà Understanding the Output

### Example Output:
```
=== STEP BY STEP CONVERSION ===
Original data stats:
  x_mean (km): 100185.1
  x_std (km): 52029.4
  y_mean (price): 5081.5
  y_std (price): 2189.9

Normalized parameters:
  theta_zero: -0.023456
  theta_one: -0.789123

Step 1 - Convert slope:
  b = theta_one * (y_std / x_std)
  b = -0.789123 * (2189.9 / 52029.4)
  b = -0.033218

Step 2 - Convert intercept:
  a = y_mean - b * x_mean + theta_zero * y_std
  a = 5081.5 - -0.033218 * 100185.1 + -0.023456 * 2189.9
  a = 8406.95

=== CONVERSION TO ORIGINAL SCALE ===
a (intercept): 8406.95
b (slope): -0.033218
Equation: price = 8406.95 + -0.033218 * km

LINEAR REGRESSION SUMMARY
============================================================
Dataset: 25 samples
Model: price = 8406.95 + -0.033218 √ó km
Training: Converged in 583 iterations

Performance Metrics:
  R¬≤ Score:  0.742
  RMSE:      ‚Ç¨1112
  MAE:       ‚Ç¨867
  MAPE:      20.3%

Key Insights:
  ‚Ä¢ Model explains 74.2% of price variation
  ‚Ä¢ Every 10,000 km reduces value by ‚Ç¨332
  ‚Ä¢ Typical prediction error: ¬±‚Ç¨1112
============================================================
```

### What This Means:
- **Base price** (a): ‚Ç¨8,406.95 (theoretical price at 0 km)
- **Depreciation rate** (b): ‚Ç¨0.033 per kilometer
- A car loses about **3.3 cents per kilometer** driven
- The model explains about **74%** of price variation
- A car with **100,000 km** is predicted to cost **‚Ç¨5,085**

## üîç Key Features

### 1. Modular Code Structure
The code is organized into specialized modules:
- `gradient.py`: Core gradient descent algorithm
- `convert.py`: Parameter conversion utilities
- `accuracy.py`: Performance evaluation metrics
- `plot.py`: Data visualization functions
- `dashboard.py`: Comprehensive visualization dashboard

### 2. Comprehensive Model Evaluation
- **R¬≤ Score**: Measures explanatory power (0-1)
- **RMSE**: Root Mean Squared Error
- **MAE**: Mean Absolute Error
- **MAPE**: Mean Absolute Percentage Error

### 3. Interactive Visualization
- **Scatter plots**: Original data visualization
- **Regression line**: Model fit visualization
- **Cost history**: Convergence monitoring
- **Prediction plots**: Visual assessment of accuracy

### 4. Command-Line Interface
- **Makefile support**: Easy installation and running
- **Prediction mode**: Quick price prediction for any mileage
- **Argument parsing**: Flexible command-line options

## üéì Educational Value

This implementation teaches:

1. **Core ML Concepts**:
   - Supervised learning
   - Cost functions
   - Optimization algorithms

2. **Mathematical Understanding**:
   - Derivatives and gradients
   - Linear algebra basics
   - Statistical normalization

3. **Programming Skills**:
   - Modular code organization
   - Data visualization
   - Command-line interfaces

4. **Real-world Application**:
   - Predicting car prices
   - Understanding depreciation
   - Market analysis
